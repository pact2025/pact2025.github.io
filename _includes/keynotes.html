<br>
<br>
<h1>Keynotes</h1>

<table border="0" style="border:none;">
  <tr>
    <td>
      <a id="keynote1"></a>
      <h2>
        Tuesday, November 4: 
        AI's Memory Challenges
      </h2>
      <i>Jae W. Lee, Seoul National University</i>
    </td>
  </tr>
  <tr>
    <td>
      Modern AI systems face a fundamental shift: memory and storage access, not computation, are becoming the primary performance bottleneck. Transformer architectures with O(L²) self-attention complexity struggle with bandwidth constraints, while agentic AI’s expanding context demands, with explicit token-space representation (KV cache), intensify capacity pressure. Memory reliability is also becoming a more critical concern, especially in the context of long-running training. This talk explores the full spectrum of challenges reshaping AI platforms and several avenues to turn these bottlenecks into breakthroughs. I’ll discuss recent solutions spanning hardware–software co-design, bandwidth-efficient algorithms, Processing-in-Memory/Near-Data Processing, and NAND flash’s evolution into specialized "AI memory" for KV caches and vector databases. This talk will explore how memory-centric design will define scalable AI infrastructure — and what opportunities lie ahead for systems researchers and architects.
    </td>
  </tr>
  <tr>
    <td>
      <p style="float: left;"><img src="../images/jaewlee.jpg" width="200px" border="1px">&nbsp;&nbsp;&nbsp;&nbsp;</p>
      Jae W. Lee is a Professor of Computer Science and Engineering and the Director of the AI Institute at Seoul National University (SNU). His research focuses on computer architecture, systems, parallel programming, and hardware security, with recent emphasis on memory-centric hardware/software co-design for AI. His work has been recognized with various awards and honors, including the IEEE Symposium on VLSI Circuits Most Frequently Cited Paper Award in 30 Years (2017), the ACM ASPLOS Most Influential Paper Award (2014), two Google Research Awards (2024, 2025), the ISCA Hall of Fame (2021), the ASPLOS Hall of Fame (2021), two IEEE Micro "Top Picks" selections (2020), the ACM ASPLOS "Highlights" Paper (2017), the HiPEAC Paper Award at ACM PLDI (2012), and the IEEE PACT Top Paper (2010). He has served as Program Co-Chair of the IEEE Micro "Top Picks" special issue (2023) and the ACM International Symposium on Memory Management (ISMM) (2024), General Chair of the International Symposium on Code Generation and Optimization (CGO) (2021, 2022), and as a PC member of numerous top-tier computer architecture and systems conferences, such as ISCA, ASPLOS, MICRO, MLSys, HPCA, SC, and Hot Chips. He spent a year as a visiting faculty researcher at Google DeepMind in Mountain View, CA, in 2022–2023. Before joining SNU, he was a research associate at Princeton University and a researcher and engineer at Parakinetics, Inc., where he conducted research on multicore software optimization. He also contributed to multiple successful VLSI implementation projects, including the Physical Uncloneable Function (PUF) and RAW Microprocessor at MIT. He received his B.S. in EE from SNU, his M.S. in EE from Stanford, and his Ph.D. in CS from MIT.
    </td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>
      <a id="keynote2"></a>
      <h2>
        Wednesday, November 5: 
        Efficient Big Graph Analytics Via Redundancy Reduction
        </h2>
        <i>Rajiv Gupta, University of California, Riverside</i>
    </td>
  </tr>
  <tr>
    <td>
      Analyses on large graphs are an increasingly important computational workload, as graph analytics is employed in many domains. Therefore, a significant amount of research in this area has focused on developing frameworks that leverage the parallelism available on various hardware platforms, ranging from a single GPU or multicore server to a cluster of servers and/or GPUs. In this talk, I will describe our work, which combines parallelism with a complementary approach that comprehensively reduces redundancy to improve scalability. Redundancy can be found and removed not only from the computation and propagation of values, but also from graph traversal and data transfer across the memory hierarchy. Our work applies redundancy reduction to two major graph analytics scenarios, involving static (fixed) graphs and evolving (changing) graphs, and achieves substantial performance improvements.
    </td>
  </tr>
  <tr>
    <td>
      <p style="float: left;"><img src="../images/rajivgupta.jpg" width="200px" border="1px">&nbsp;&nbsp;&nbsp;&nbsp;</p>
      Rajiv is a Distinguished Professor and the Amrik Singh Poonian Chair in Computer Science at the University of California, Riverside. His research interests include compilers, architectures, and runtimes for parallel systems. He has co-authored over 300 papers, with more than 16,000 citations and an h-index of 69. He has supervised 42 Ph.D. dissertations, including those of two ACM SIGPLAN Outstanding Doctoral Dissertation Award winners. Rajiv is a Fellow of the IEEE, ACM, and AAAS. He received the NSF Presidential Young Investigator Award and the UCR Doctoral Dissertation Advisor/Mentor Award. He has chaired several major ACM/IEEE conferences, including FCRC, PLDI, HPCA, ASPLOS, PPoPP, and PACT. Rajiv also served as a member of a technical advisory group on networking and information technology established by the U.S. President’s Council of Advisors on Science and Technology.
    </td>
  </tr>
