  <td>
    <b>Session 1: <a href="https://conferences.computer.org/pactpub25/#!/toc/0#LLMSystemsatScale" target="_blank">LLM Systems at Scale</a></b>
    <br>
    <b>Chair:</b> Lawrence Rauchwerger, University of Illinos Urbana-Champaigni<br>
    <ul>
      <li><a href="https://conferences.computer.org/pactpub25/pdfs/PACT2025-3gKGRwA9U4f6t0U4gWpYze/829500a001/829500a001.pdf" target="_blank">Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled KV-Cache Management Beyond GPU Limits</a><br>
          <i>Dowon Kim (Hanyang University), Minjae Lee (Hanyang University), Janghyeon Kim (Hanyang University), HyuckSung Kwon (Hanyang University), Hyeonggyu Jeong (Hanyang University), Sang-Soo Park (Samsung Electronics), Minyong Yoon (Samsung Electronics), Si-Dong Roh (Samsung Electronics), Jinin So (Samsung Electronics), Jungwook Choi (Hanyang University), and Yongsuk Kwon (Samsung Electronics)</i>
      </li>
      <li><a href="https://conferences.computer.org/pactpub25/pdfs/PACT2025-3gKGRwA9U4f6t0U4gWpYze/829500a014/829500a014.pdf" target="_blank">SPipe: Hybrid GPU and CPU Pipeline for Training LLMs under Memory Pressure</a><br>
          <i>Junyeol Ryu (Seoul National University), Yujin Jeong (Seoul National University), Daeyoung Park (Seoul National University), Jinpyo Kim (Seoul National University), Heehoon Kim (Seoul National University), and Jaejin Lee (Seoul National University)</i>
      </li>
      <li><a href="https://conferences.computer.org/pactpub25/pdfs/PACT2025-3gKGRwA9U4f6t0U4gWpYze/829500a030/829500a030.pdf" target="_blank">ScaleMoE: A Fast and Scalable Distributed Training Framework for Large-Scale Mixture-of-Experts Models</a><br>
          <i>Seohong Choi (Sungkyunkwan University), Huize Hong (Sungkyunkwan University), Tae Hee Han (Sungkyunkwan University), and Joonsung Kim (Sungkyunkwan University)</i>
      </li>
      <li><a href="https://conferences.computer.org/pactpub25/pdfs/PACT2025-3gKGRwA9U4f6t0U4gWpYze/829500a043/829500a043.pdf" target="_blank">LibraPIM: Dynamic Load Rebalancing to Maximize Utilization in PIM-Assisted LLM Inference Systems</a><br>
          <i>Hyeongjun Cho (Sungkyunkwan University), Yoonho Jang (Sungkyunkwan University), Hyungi Kim (Sungkyunkwan University), Seongwook Kim (Sungkyunkwan University), Keewon Kwon (Sungkyunkwan University), Gwangsun Kim (POSTECH), and Seokin Hong (Sungkyunkwan University)</i>
      </li>
      <li><a href="https://conferences.computer.org/pactpub25/pdfs/PACT2025-3gKGRwA9U4f6t0U4gWpYze/829500a057/829500a057.pdf" target="_blank">Doppeladler: Adaptive Tensor Parallelism for Latency-Critical LLM Deployment on CPU-GPU Integrated End-User Device</a><br>
          <i>Jiazhi Jiang (Sun Yat-sen University), Xiao Liu (Sun Yat-sen University), Jiangsu Du (Sun Yat-sen University), Dan Huang (Sun Yat-sen University), and Yutong Lu (Sun Yat-sen University)</i>
      </li>
    </ul>
  </td>
